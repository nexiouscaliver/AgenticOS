# AgenticOS FastAPI Backend - API Documentation

## Table of Contents
- [Overview](#overview)
- [Architecture](#architecture)
- [Getting Started](#getting-started)
- [Authentication & Security](#authentication--security)
- [API Endpoints](#api-endpoints)
  - [System Endpoints](#system-endpoints)
  - [Agent Endpoints](#agent-endpoints)
  - [Team Endpoints](#team-endpoints)
  - [Workflow Endpoints](#workflow-endpoints)
- [Request/Response Formats](#requestresponse-formats)
- [Error Handling](#error-handling)
- [Configuration](#configuration)
- [Best Practices](#best-practices)
- [Examples](#examples)

---

## Overview

AgenticOS is a production-ready FastAPI backend that serves AI agents, teams, and workflows through a RESTful API. The application is built using the **Agno Framework 2.0.4**, which automatically generates a FastAPI application with standardized endpoints for multi-agent systems.

### Key Features

- **6 Specialized AI Agents**: Research, content writing, fact-checking, SEO optimization, documentation assistance
- **Multi-Agent Teams**: Coordinated research team with parallel execution
- **Automated Workflows**: Blog writing workflows with quality gates
- **Multi-Model Support**: OpenAI, DeepSeek, GLM, Google Gemini
- **Cost Optimization**: Intelligent model selection (40% cost reduction)
- **PostgreSQL Integration**: Agent memory, conversation history, knowledge bases
- **Vector Search**: pgvector for RAG (Retrieval-Augmented Generation)
- **Production-Ready**: Docker support, health checks, comprehensive testing

### Tech Stack

- **Framework**: FastAPI 0.116.1
- **Server**: Uvicorn 0.35.0 (ASGI)
- **AI Framework**: Agno 2.0.4
- **Database**: PostgreSQL 16 + pgvector 0.4.1
- **ORM**: SQLAlchemy 2.0.41
- **LLM Providers**: OpenAI, DeepSeek, GLM, Google Gemini

---

## Architecture

### Application Structure

```
AgenticOS (FastAPI Application)
├── AgentOS Instance
│   ├── 6 Specialized Agents
│   │   ├── Advanced Web Research Agent
│   │   ├── Agno Documentation Expert
│   │   ├── Research Analyst Agent
│   │   ├── Content Writer Agent
│   │   ├── Fact Checker Agent
│   │   └── SEO Optimizer Agent
│   ├── 1 Research Team
│   │   └── Comprehensive Research Team (4 coordinated agents)
│   └── 2 Workflows
│       ├── Comprehensive Blog Writing Workflow (7 steps)
│       └── Simple Blog Writing Workflow (3 steps)
├── PostgreSQL Database
│   ├── 8 Agent-specific storage instances
│   ├── Knowledge bases with vector embeddings
│   └── Workflow state management
└── FastAPI Routes (Auto-generated by Agno)
```

### How AgentOS Generates the FastAPI App

```python
# app/main.py
from agno.os import AgentOS

# Create AgentOS instance with agents, teams, workflows
agent_os = AgentOS(
    os_id="agenticos-enhanced",
    agents=[...],      # List of 6 Agent instances
    teams=[...],       # List of Team instances
    workflows=[...],   # List of Workflow instances
    config="config.yaml"
)

# Get auto-generated FastAPI application
app = agent_os.get_app()
```

The `get_app()` method automatically creates:
- REST endpoints for each agent, team, and workflow
- Request/response validation using Pydantic
- OpenAPI/Swagger documentation
- Health check endpoints
- Error handling middleware

---

## Getting Started

### Prerequisites

- Python 3.11+
- Docker & Docker Compose
- PostgreSQL 16 (or Docker setup provided)
- API keys for LLM providers

### Environment Setup

1. **Clone and navigate to the project:**
```bash
git clone <repository-url>
cd AgenticOS/agent-infra-docker
```

2. **Install dependencies:**
```bash
# Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Run setup script
./scripts/dev_setup.sh
source .venv/bin/activate
```

3. **Set environment variables:**
```bash
# Required
export OPENAI_API_KEY="your-openai-api-key"

# Optional (for alternative models)
export DEEPSEEK_API_KEY="your-deepseek-key"
export GLM_API_KEY="your-glm-key"
export GOOGLE_API_KEY="your-google-key"

# Database configuration (defaults provided)
export DB_HOST="localhost"
export DB_PORT="5432"
export DB_USER="ai"
export DB_PASS="ai"
export DB_DATABASE="ai"

# Application settings
export DEBUG_MODE="false"
```

### Running the Application

**Option 1: Docker Compose (Recommended)**
```bash
docker compose up -d --build

# Access the API
curl http://localhost:8000/health
```

**Option 2: Development Server**
```bash
# Start PostgreSQL separately (or use Docker Compose for DB only)
docker compose up -d pgvector

# Start FastAPI server
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### Accessing the API

- **API Base URL**: `http://localhost:8000`
- **Interactive Documentation**: `http://localhost:8000/docs` (Swagger UI)
- **OpenAPI Schema**: `http://localhost:8000/openapi.json`
- **Alternative Docs**: `http://localhost:8000/redoc` (ReDoc)

---

## Authentication & Security

### Current Implementation

**Public API**: The current implementation does not include authentication. All endpoints are publicly accessible.

### Security Considerations for Production

For production deployments, consider implementing:

1. **API Key Authentication**
   - Add API key validation middleware
   - Use environment variables for key storage
   - Implement rate limiting per API key

2. **JWT (JSON Web Tokens)**
   - User authentication with token-based access
   - Role-based access control (RBAC)
   - Token expiration and refresh mechanisms

3. **OAuth 2.0**
   - Third-party authentication providers
   - Secure authorization flows

4. **CORS Configuration**
   ```python
   from fastapi.middleware.cors import CORSMiddleware

   app.add_middleware(
       CORSMiddleware,
       allow_origins=["https://yourdomain.com"],
       allow_credentials=True,
       allow_methods=["*"],
       allow_headers=["*"],
   )
   ```

5. **HTTPS/TLS**
   - Use reverse proxy (Nginx, Traefik)
   - SSL certificate management
   - Force HTTPS redirects

6. **Rate Limiting**
   - Prevent abuse and DoS attacks
   - Per-IP or per-user limits
   - Use libraries like `slowapi`

### Database Security

- Credentials managed via environment variables
- Connection string construction in `app/db/url.py`
- PostgreSQL connection pooling with `pool_pre_ping=True`
- Use managed database services in production (RDS, Cloud SQL)

---

## API Endpoints

### Base URL Structure

```
http://localhost:8000/{resource}/{id}/{action}
```

### System Endpoints

#### 1. Health Check

**Endpoint**: `GET /health`

**Description**: Check if the API is running and responsive.

**Request**: None

**Response**:
```json
{
  "status": "healthy"
}
```

**HTTP Status**: `200 OK`

**Example**:
```bash
curl http://localhost:8000/health
```

---

#### 2. List All Agents

**Endpoint**: `GET /agents`

**Description**: Retrieve a list of all available agents with their metadata.

**Request**: None

**Response**:
```json
{
  "agents": [
    {
      "id": "advanced-web-research-agent",
      "name": "Advanced Web Research Agent",
      "description": "Elite investigative research agent...",
      "model": "deepseek-chat",
      "tools": ["DuckDuckGo"],
      "capabilities": [...]
    },
    {
      "id": "agno-documentation-expert",
      "name": "Agno Documentation Expert",
      "description": "Expert in Agno Framework...",
      "model": "gpt-4o-mini",
      "tools": ["DuckDuckGo"],
      "knowledge_enabled": true
    },
    // ... 4 more agents
  ]
}
```

**HTTP Status**: `200 OK`

**Example**:
```bash
curl http://localhost:8000/agents
```

---

#### 3. List All Teams

**Endpoint**: `GET /teams`

**Description**: Retrieve a list of all available multi-agent teams.

**Response**:
```json
{
  "teams": [
    {
      "id": "comprehensive-research-team",
      "name": "Comprehensive Research Team",
      "description": "Multi-agent research coordination system",
      "members": [
        "advanced-web-research-agent",
        "research-analyst-agent",
        "fact-checker-agent",
        "secondary-web-research-agent"
      ],
      "coordination": "parallel_with_synthesis"
    }
  ]
}
```

**HTTP Status**: `200 OK`

**Example**:
```bash
curl http://localhost:8000/teams
```

---

#### 4. List All Workflows

**Endpoint**: `GET /workflows`

**Description**: Retrieve a list of all available automated workflows.

**Response**:
```json
{
  "workflows": [
    {
      "id": "comprehensive-blog-workflow",
      "name": "Comprehensive Blog Writing Workflow",
      "description": "End-to-end blog creation with research, SEO, and fact-checking",
      "steps": 7,
      "features": ["parallel_execution", "conditional_logic", "quality_gates"]
    },
    {
      "id": "simple-blog-workflow",
      "name": "Simple Blog Writing Workflow",
      "description": "Streamlined blog creation workflow",
      "steps": 3
    }
  ]
}
```

**HTTP Status**: `200 OK`

**Example**:
```bash
curl http://localhost:8000/workflows
```

---

#### 5. OpenAPI Documentation

**Endpoint**: `GET /openapi.json`

**Description**: Retrieve the OpenAPI specification for the API.

**Response**: JSON OpenAPI 3.0 schema

**HTTP Status**: `200 OK`

---

#### 6. Interactive API Documentation

**Endpoint**: `GET /docs`

**Description**: Access Swagger UI for interactive API testing.

**Response**: HTML (Swagger UI interface)

**HTTP Status**: `200 OK`

---

### Agent Endpoints

All 6 agents share the same endpoint pattern for chat interactions.

#### Agent Chat

**Endpoint**: `POST /agents/{agent_id}/chat`

**Description**: Send a message to a specific agent and receive a response.

**Available Agent IDs**:
- `advanced-web-research-agent`
- `agno-documentation-expert`
- `research-analyst-agent`
- `content-writer-agent`
- `fact-checker-agent`
- `seo-optimizer-agent`

**Request Body**:
```json
{
  "message": "Your question or prompt",
  "stream": false,
  "session_id": "optional-session-id-for-conversation-continuity"
}
```

**Request Fields**:
| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `message` | string | Yes | User's question or prompt for the agent |
| `stream` | boolean | No | Enable streaming response (default: false) |
| `session_id` | string | No | Session identifier for conversation continuity |

**Response** (Non-streaming):
```json
{
  "id": "response-uuid",
  "agent_id": "advanced-web-research-agent",
  "content": "Agent's detailed response...",
  "metadata": {
    "model": "deepseek-chat",
    "tokens_used": 1250,
    "response_time_ms": 2350,
    "timestamp": "2025-11-13T10:30:45.123Z"
  },
  "session_id": "session-uuid"
}
```

**Response Fields**:
| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Unique response identifier |
| `agent_id` | string | Agent that generated the response |
| `content` | string | Agent's response (may include markdown) |
| `metadata` | object | Additional information about the response |
| `session_id` | string | Session identifier for conversation tracking |

**HTTP Status**:
- `200 OK` - Success
- `400 Bad Request` - Invalid request format
- `404 Not Found` - Agent ID not found
- `500 Internal Server Error` - Processing error

**Example - Web Research Agent**:
```bash
curl -X POST http://localhost:8000/agents/advanced-web-research-agent/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Research the latest developments in quantum computing",
    "stream": false
  }'
```

**Example Response**:
```json
{
  "id": "resp_abc123",
  "agent_id": "advanced-web-research-agent",
  "content": "# Latest Developments in Quantum Computing\n\n## Executive Summary\nQuantum computing has seen significant advances in 2025...\n\n## Key Findings\n1. **IBM Quantum System Two**: Released with 1,121 qubits...\n2. **Google's Breakthrough**: Error correction milestone achieved...\n\n## Sources\n- IBM Research, January 2025\n- Nature Physics, March 2025\n- MIT Technology Review, February 2025\n\n**Research Confidence**: High\n**Sources Consulted**: 8 sources across 3 categories",
  "metadata": {
    "model": "deepseek-chat",
    "tokens_used": 1850,
    "response_time_ms": 4200,
    "timestamp": "2025-11-13T10:30:45.123Z",
    "sources_count": 8
  },
  "session_id": "sess_xyz789"
}
```

---

**Example - Agno Documentation Expert**:
```bash
curl -X POST http://localhost:8000/agents/agno-documentation-expert/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "How do I create an Agno agent with custom tools?",
    "stream": false
  }'
```

**Example Response**:
```json
{
  "id": "resp_def456",
  "agent_id": "agno-documentation-expert",
  "content": "# Creating an Agno Agent with Custom Tools\n\n## Step-by-Step Implementation\n\n```python\nfrom agno.agent import Agent\nfrom agno.models.openai import OpenAIChat\nfrom agno.tools.base import Tool\n\nclass CustomCalculatorTool(Tool):\n    def __init__(self):\n        super().__init__(name=\"calculator\")\n    \n    def run(self, expression: str) -> str:\n        return str(eval(expression))\n\nagent = Agent(\n    id=\"math-agent\",\n    model=OpenAIChat(id=\"gpt-4o-mini\"),\n    tools=[CustomCalculatorTool()],\n    instructions=\"You are a helpful math assistant.\"\n)\n\nresponse = agent.run(\"Calculate 25 * 48\")\nprint(response.content)\n```\n\n## Best Practices\n1. Always validate tool inputs...\n2. Handle exceptions gracefully...\n3. Provide clear tool descriptions...",
  "metadata": {
    "model": "gpt-4o-mini",
    "tokens_used": 650,
    "response_time_ms": 1800,
    "timestamp": "2025-11-13T10:32:15.456Z",
    "knowledge_search_used": true
  },
  "session_id": "sess_abc123"
}
```

---

**Example - Content Writer Agent**:
```bash
curl -X POST http://localhost:8000/agents/content-writer-agent/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Write an engaging introduction for a blog about sustainable technology",
    "stream": false
  }'
```

---

**Example - Session Continuity**:
```bash
# First message
curl -X POST http://localhost:8000/agents/research-analyst-agent/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Analyze the impact of AI on healthcare",
    "stream": false
  }'
# Returns: { "session_id": "sess_123" }

# Follow-up message in same session
curl -X POST http://localhost:8000/agents/research-analyst-agent/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What about the ethical concerns?",
    "stream": false,
    "session_id": "sess_123"
  }'
```

---

### Team Endpoints

#### Team Chat

**Endpoint**: `POST /teams/{team_id}/chat`

**Description**: Coordinate multiple agents to work together on a complex task.

**Available Team IDs**:
- `comprehensive-research-team`

**Request Body**:
```json
{
  "message": "Your complex research question",
  "stream": false,
  "session_id": "optional-session-id"
}
```

**Request Fields**: Same as Agent Chat

**Response**:
```json
{
  "id": "team_resp_abc123",
  "team_id": "comprehensive-research-team",
  "content": "# TEAM RESEARCH REPORT\n\n## Executive Summary\n- **Research Objective**: [Clear problem statement]\n- **Key Finding**: [Primary conclusion with confidence level]\n- **Team Assessment**: [Overall quality rating]\n\n## Detailed Findings\n\n### Primary Research (Web Research Agent)\n[Initial discoveries and source analysis]\n\n### Academic Analysis (Research Analyst)\n[Patterns, trends, and expert interpretation]\n\n### Fact Verification (Fact Checker)\n[Accuracy validation and confidence scores]\n\n### Supplementary Research (Secondary Web Agent)\n[Additional perspectives and updates]\n\n## Integrated Analysis\n- **Consensus Findings**: [Areas where all sources agree]\n- **Conflicting Evidence**: [Disagreements and alternatives]\n- **Emerging Patterns**: [Trends identified by the team]\n\n## Source Documentation\n- **Primary Sources**: 12 sources across 4 categories\n- **Quality Assessment**: A-rated: 8, B-rated: 4\n- **Verification Status**: 95% claims verified\n\n## Team Quality Indicators\n- **Research Confidence**: A (High)\n- **Source Coverage**: Comprehensive\n- **Team Consensus**: Strong agreement",
  "metadata": {
    "team_members": 4,
    "coordination_time_ms": 12500,
    "total_sources": 12,
    "verification_rate": 0.95,
    "timestamp": "2025-11-13T10:45:30.789Z"
  },
  "session_id": "team_sess_xyz789"
}
```

**HTTP Status**: Same as Agent Chat

**Timeout Considerations**: Team coordination takes longer (30-120 seconds typical)

**Example**:
```bash
curl -X POST http://localhost:8000/teams/comprehensive-research-team/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Research the environmental and economic impact of renewable energy adoption in developing countries",
    "stream": false
  }' \
  --max-time 120
```

---

### Workflow Endpoints

#### Workflow Execution

**Endpoint**: `POST /workflows/{workflow_id}/run`

**Description**: Execute a multi-step automated workflow with quality gates and parallel processing.

**Available Workflow IDs**:
- `comprehensive-blog-workflow` (7 steps: analysis → research → planning → writing → conditional check → parallel enhancement → integration)
- `simple-blog-workflow` (3 steps: research → writing → SEO)

**Request Body**:
```json
{
  "message": "Blog topic or content request",
  "stream": false,
  "additional_data": {
    "target_audience": "professionals",
    "seo_priority": "high",
    "content_type": "how-to",
    "research_depth": "comprehensive"
  }
}
```

**Request Fields**:
| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `message` | string | Yes | Blog topic or content request |
| `stream` | boolean | No | Enable streaming response |
| `additional_data` | object | No | Workflow-specific parameters |

**Additional Data Options**:
| Field | Type | Values | Description |
|-------|------|--------|-------------|
| `target_audience` | string | "professionals", "beginners", "technical", etc. | Content audience |
| `seo_priority` | string | "low", "medium", "high" | SEO optimization level |
| `content_type` | string | "how-to", "listicle", "opinion", "case-study" | Blog format |
| `research_depth` | string | "basic", "moderate", "comprehensive" | Research thoroughness |

**Response**:
```json
{
  "id": "workflow_resp_abc123",
  "workflow_id": "comprehensive-blog-workflow",
  "status": "completed",
  "steps_executed": [
    {
      "step_name": "Topic Analysis",
      "status": "completed",
      "duration_ms": 2500
    },
    {
      "step_name": "Research Execution",
      "status": "completed",
      "duration_ms": 15000
    },
    {
      "step_name": "Content Planning",
      "status": "completed",
      "duration_ms": 3000
    },
    {
      "step_name": "Blog Writing",
      "status": "completed",
      "duration_ms": 8000
    },
    {
      "step_name": "Additional Research Check",
      "status": "skipped",
      "reason": "content_sufficient"
    },
    {
      "step_name": "Content Enhancement (Parallel)",
      "status": "completed",
      "duration_ms": 6000,
      "sub_steps": [
        {"name": "SEO Optimization", "status": "completed"},
        {"name": "Fact Checking", "status": "completed"}
      ]
    },
    {
      "step_name": "Final Integration",
      "status": "completed",
      "duration_ms": 2000
    }
  ],
  "content": "## Final Blog Post Ready\n\n**Topic**: [Your topic]\n\n# [SEO-Optimized Title]\n\n[Compelling introduction with hook]\n\n## [H2 Heading 1]\n[Content with natural keyword integration]\n\n## [H2 Heading 2]\n[Data-driven content with citations]\n\n## [H2 Heading 3]\n[Actionable insights and examples]\n\n## Conclusion\n[Summary and call-to-action]\n\n---\n\n### SEO Metadata\n- **Title Tag**: [60-character optimized title]\n- **Meta Description**: [155-character compelling description]\n- **Primary Keywords**: [keyword1, keyword2, keyword3]\n- **Target Length**: 1,500 words\n- **Readability Score**: High\n\n### Quality Indicators\n- SEO Optimized: ✓\n- Fact Checked: ✓\n- Content Quality: Professional\n- Status: Ready for Publication",
  "metadata": {
    "total_duration_ms": 36500,
    "steps_completed": 6,
    "steps_skipped": 1,
    "research_sources": 8,
    "fact_check_confidence": "high",
    "seo_score": 92,
    "timestamp": "2025-11-13T11:00:45.123Z"
  }
}
```

**HTTP Status**: Same as Agent Chat

**Timeout Considerations**: Workflows can take 30-180 seconds depending on complexity

**Example - Comprehensive Workflow**:
```bash
curl -X POST http://localhost:8000/workflows/comprehensive-blog-workflow/run \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Write a comprehensive blog about the benefits and challenges of remote work in 2025",
    "stream": false,
    "additional_data": {
      "target_audience": "business professionals",
      "seo_priority": "high",
      "content_type": "analysis",
      "research_depth": "comprehensive"
    }
  }' \
  --max-time 180
```

**Example - Simple Workflow**:
```bash
curl -X POST http://localhost:8000/workflows/simple-blog-workflow/run \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Write a quick blog about productivity tips",
    "stream": false
  }' \
  --max-time 60
```

---

## Request/Response Formats

### Common Request Format

All POST endpoints accept JSON with the following structure:

```typescript
{
  message: string;          // Required: User's input
  stream: boolean;          // Optional: Enable streaming (default: false)
  session_id?: string;      // Optional: Session for continuity
  additional_data?: object; // Optional: Workflow-specific parameters
}
```

### Common Response Format

```typescript
{
  id: string;              // Unique response identifier
  content: string;         // Agent/team/workflow output (markdown)
  metadata: {
    timestamp: string;     // ISO 8601 timestamp
    [key: string]: any;    // Additional metadata
  };
  session_id?: string;     // Session identifier
  status?: string;         // Workflow status
}
```

### Streaming Responses

When `stream: true` is set, responses are sent as Server-Sent Events (SSE):

```
event: message
data: {"chunk": "First part of response..."}

event: message
data: {"chunk": "Second part..."}

event: done
data: {"id": "resp_123", "metadata": {...}}
```

**Client Example**:
```python
import requests

response = requests.post(
    "http://localhost:8000/agents/content-writer-agent/chat",
    json={"message": "Write a paragraph", "stream": True},
    stream=True
)

for line in response.iter_lines():
    if line:
        print(line.decode('utf-8'))
```

---

## Error Handling

### HTTP Status Codes

| Code | Meaning | Description |
|------|---------|-------------|
| `200` | OK | Request succeeded |
| `400` | Bad Request | Invalid request format or parameters |
| `404` | Not Found | Agent/team/workflow ID not found |
| `422` | Unprocessable Entity | Validation error (Pydantic) |
| `500` | Internal Server Error | Server-side processing error |
| `503` | Service Unavailable | Database or external service unavailable |

### Error Response Format

```json
{
  "detail": "Error message describing what went wrong",
  "error_type": "ValidationError",
  "status_code": 400,
  "timestamp": "2025-11-13T10:30:45.123Z"
}
```

### Common Error Scenarios

**1. Invalid Agent ID**
```json
{
  "detail": "Agent 'invalid-agent-id' not found",
  "status_code": 404
}
```

**2. Missing Required Field**
```json
{
  "detail": [
    {
      "loc": ["body", "message"],
      "msg": "field required",
      "type": "value_error.missing"
    }
  ],
  "status_code": 422
}
```

**3. Database Connection Error**
```json
{
  "detail": "Database connection failed",
  "error_type": "DatabaseError",
  "status_code": 503
}
```

**4. LLM API Error**
```json
{
  "detail": "OpenAI API rate limit exceeded",
  "error_type": "RateLimitError",
  "status_code": 500,
  "retry_after": 60
}
```

### Error Handling Best Practices

**Client-Side**:
```python
import requests
from requests.exceptions import Timeout, RequestException

try:
    response = requests.post(
        "http://localhost:8000/agents/web-agent/chat",
        json={"message": "Research quantum computing"},
        timeout=60
    )
    response.raise_for_status()  # Raise exception for 4xx/5xx

    data = response.json()
    print(data['content'])

except Timeout:
    print("Request timed out. Agent may be processing a complex task.")
except RequestException as e:
    print(f"Request failed: {e}")
```

---

## Configuration

### Environment Variables

#### Required

| Variable | Description | Example |
|----------|-------------|---------|
| `OPENAI_API_KEY` | OpenAI API key for GPT models | `sk-...` |

#### Optional - LLM Providers

| Variable | Description | Default |
|----------|-------------|---------|
| `DEEPSEEK_API_KEY` | DeepSeek API key | None |
| `GLM_API_KEY` | GLM-4.5 API key | None |
| `GOOGLE_API_KEY` | Google Gemini API key | None |

#### Database Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `DB_DRIVER` | Database driver | `postgresql+psycopg` |
| `DB_HOST` | Database host | `localhost` |
| `DB_PORT` | Database port | `5432` |
| `DB_USER` | Database username | `ai` |
| `DB_PASS` | Database password | `ai` |
| `DB_DATABASE` | Database name | `ai` |

#### Application Settings

| Variable | Description | Default |
|----------|-------------|---------|
| `DEBUG_MODE` | Enable debug logging | `false` |
| `WAIT_FOR_DB` | Wait for database on startup | `True` |
| `PRINT_ENV_ON_LOAD` | Print config at startup | `True` |

### Configuration File

**Location**: `app/config.yaml`

```yaml
chat:
  quick_prompts:
    web-search-agent:
      - "What can you do?"
      - "What is currently happening in France?"
      - "What is the latest news on the stock market?"
    agno-assist:
      - "What can you do?"
      - "What is the latest news on Agno?"
      - "Tell me about Agno's AgentOS"
```

### Docker Compose Configuration

**File**: `compose.yaml`

```yaml
services:
  pgvector:
    image: agnohq/pgvector:16
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${DB_USER:-ai}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-ai}
      POSTGRES_DB: ${DB_NAME:-ai}

  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      DB_HOST: pgvector
      DB_PORT: 5432
    depends_on:
      - pgvector
```

---

## Best Practices

### 1. Timeout Configuration

Set appropriate timeouts based on endpoint type:

| Endpoint Type | Recommended Timeout |
|---------------|---------------------|
| Agent Chat | 30-60 seconds |
| Team Chat | 60-120 seconds |
| Workflows | 120-180 seconds |
| System Endpoints | 5 seconds |

```python
import requests

# Agent request
response = requests.post(url, json=data, timeout=60)

# Team request
response = requests.post(url, json=data, timeout=120)

# Workflow request
response = requests.post(url, json=data, timeout=180)
```

### 2. Session Management

Use session IDs for conversation continuity:

```python
# Store session_id from first response
first_response = requests.post(url, json={"message": "Hello"}).json()
session_id = first_response.get("session_id")

# Use in follow-up requests
follow_up = requests.post(
    url,
    json={"message": "Continue our discussion", "session_id": session_id}
)
```

### 3. Error Retry Logic

Implement exponential backoff for transient errors:

```python
import time

def make_request_with_retry(url, data, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = requests.post(url, json=data, timeout=60)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                raise
            wait_time = 2 ** attempt  # 1s, 2s, 4s
            time.sleep(wait_time)
```

### 4. Streaming for Long Responses

Use streaming for better UX:

```python
def stream_response(url, message):
    response = requests.post(
        url,
        json={"message": message, "stream": True},
        stream=True
    )

    for line in response.iter_lines():
        if line:
            yield line.decode('utf-8')
```

### 5. Cost Optimization

Monitor token usage via metadata:

```python
response = requests.post(url, json={"message": "..."}).json()
tokens_used = response['metadata'].get('tokens_used', 0)
cost_estimate = tokens_used / 1000 * 0.00014  # DeepSeek rate

print(f"Tokens: {tokens_used}, Estimated cost: ${cost_estimate:.6f}")
```

### 6. Rate Limiting

Implement client-side rate limiting:

```python
from time import sleep
from collections import deque
from datetime import datetime, timedelta

class RateLimiter:
    def __init__(self, max_requests, time_window):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = deque()

    def allow_request(self):
        now = datetime.now()
        cutoff = now - timedelta(seconds=self.time_window)

        # Remove old requests
        while self.requests and self.requests[0] < cutoff:
            self.requests.popleft()

        if len(self.requests) < self.max_requests:
            self.requests.append(now)
            return True
        return False

# Usage: 10 requests per minute
limiter = RateLimiter(max_requests=10, time_window=60)

if limiter.allow_request():
    response = requests.post(url, json=data)
else:
    print("Rate limit exceeded, waiting...")
    sleep(5)
```

---

## Examples

### Python Client Examples

#### Basic Agent Interaction

```python
import requests

def chat_with_agent(agent_id, message):
    """Send a message to an agent and get response"""
    url = f"http://localhost:8000/agents/{agent_id}/chat"
    response = requests.post(
        url,
        json={"message": message, "stream": False},
        timeout=60
    )
    response.raise_for_status()
    return response.json()

# Research with web agent
result = chat_with_agent(
    "advanced-web-research-agent",
    "What are the latest developments in renewable energy?"
)
print(result['content'])
```

#### Team Research

```python
def research_with_team(question):
    """Coordinate research team for comprehensive analysis"""
    url = "http://localhost:8000/teams/comprehensive-research-team/chat"
    response = requests.post(
        url,
        json={"message": question, "stream": False},
        timeout=120
    )
    response.raise_for_status()
    return response.json()

# Complex research question
result = research_with_team(
    "Analyze the economic and environmental impact of electric vehicles"
)
print(result['content'])
print(f"\nResearch confidence: {result['metadata'].get('verification_rate')}")
```

#### Blog Writing Workflow

```python
def create_blog_post(topic, target_audience="professionals", seo_priority="high"):
    """Generate a complete blog post using workflow"""
    url = "http://localhost:8000/workflows/comprehensive-blog-workflow/run"
    response = requests.post(
        url,
        json={
            "message": topic,
            "stream": False,
            "additional_data": {
                "target_audience": target_audience,
                "seo_priority": seo_priority,
                "research_depth": "comprehensive"
            }
        },
        timeout=180
    )
    response.raise_for_status()
    return response.json()

# Generate blog post
blog = create_blog_post(
    "The Future of Work: Remote vs Hybrid Models",
    target_audience="business leaders",
    seo_priority="high"
)

print(blog['content'])
print(f"\nSEO Score: {blog['metadata'].get('seo_score')}")
print(f"Steps completed: {blog['metadata'].get('steps_completed')}")
```

#### Conversation with Session

```python
class AgentConversation:
    """Maintain conversation context with an agent"""

    def __init__(self, agent_id):
        self.agent_id = agent_id
        self.session_id = None
        self.base_url = f"http://localhost:8000/agents/{agent_id}/chat"

    def send_message(self, message):
        """Send message and maintain session"""
        payload = {"message": message, "stream": False}
        if self.session_id:
            payload["session_id"] = self.session_id

        response = requests.post(self.base_url, json=payload, timeout=60)
        response.raise_for_status()
        result = response.json()

        # Store session ID from first response
        if not self.session_id:
            self.session_id = result.get("session_id")

        return result['content']

# Usage
conv = AgentConversation("research-analyst-agent")
print(conv.send_message("Analyze the impact of AI on healthcare"))
print(conv.send_message("What are the main ethical concerns?"))
print(conv.send_message("How can we address these concerns?"))
```

### JavaScript/TypeScript Examples

#### Fetch API

```javascript
async function chatWithAgent(agentId, message) {
    const response = await fetch(
        `http://localhost:8000/agents/${agentId}/chat`,
        {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ message, stream: false })
        }
    );

    if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
    }

    return await response.json();
}

// Usage
const result = await chatWithAgent(
    'content-writer-agent',
    'Write an introduction about AI ethics'
);
console.log(result.content);
```

#### Axios

```typescript
import axios from 'axios';

interface ChatRequest {
    message: string;
    stream?: boolean;
    session_id?: string;
}

interface ChatResponse {
    id: string;
    content: string;
    metadata: Record<string, any>;
    session_id?: string;
}

const api = axios.create({
    baseURL: 'http://localhost:8000',
    timeout: 60000,
    headers: { 'Content-Type': 'application/json' }
});

async function chatWithAgent(
    agentId: string,
    message: string
): Promise<ChatResponse> {
    const response = await api.post<ChatResponse>(
        `/agents/${agentId}/chat`,
        { message, stream: false }
    );
    return response.data;
}

// Usage
try {
    const result = await chatWithAgent(
        'seo-optimizer-agent',
        'Optimize this content for search engines: "AI in Healthcare"'
    );
    console.log(result.content);
} catch (error) {
    console.error('API error:', error);
}
```

### cURL Examples

#### Web Research
```bash
curl -X POST http://localhost:8000/agents/advanced-web-research-agent/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Research the latest trends in artificial intelligence",
    "stream": false
  }' \
  | jq '.content'
```

#### Fact Checking
```bash
curl -X POST http://localhost:8000/agents/fact-checker-agent/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Verify: The global average temperature has increased by 1.1°C since pre-industrial times",
    "stream": false
  }' \
  | jq '.content'
```

#### Team Research with Timeout
```bash
curl -X POST http://localhost:8000/teams/comprehensive-research-team/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Comprehensive analysis of blockchain technology adoption",
    "stream": false
  }' \
  --max-time 120 \
  | jq '.'
```

#### Blog Workflow
```bash
curl -X POST http://localhost:8000/workflows/simple-blog-workflow/run \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Write about productivity hacks for remote workers",
    "stream": false
  }' \
  --max-time 90 \
  | jq '.content'
```

---

## Advanced Topics

### Custom Agent Integration

To add custom agents to the system:

1. Create agent file in `app/agents/`:
```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.db.postgres import PostgresDb
from db.session import db_url

def get_custom_agent(model_id: str = "gpt-4o-mini", debug_mode: bool = False):
    return Agent(
        id="custom-agent",
        name="Custom Agent",
        model=OpenAIChat(id=model_id),
        description="Your agent description",
        instructions="Your detailed instructions",
        db=PostgresDb(id="custom-agent-storage", db_url=db_url),
    )
```

2. Register in `app/main.py`:
```python
from agents.custom_agent import get_custom_agent

agents = [
    # ... existing agents
    get_custom_agent(model_id="gpt-4o-mini", debug_mode=debug_mode)
]
```

3. Restart the application - new endpoint automatically created:
```
POST /agents/custom-agent/chat
```

### Performance Monitoring

Monitor API performance:

```python
import time
import requests

def timed_request(url, data):
    start = time.time()
    response = requests.post(url, json=data)
    elapsed = time.time() - start

    result = response.json()
    return {
        'response': result,
        'client_time': elapsed,
        'server_time': result['metadata'].get('response_time_ms', 0) / 1000,
        'network_overhead': elapsed - (result['metadata'].get('response_time_ms', 0) / 1000)
    }

metrics = timed_request(
    "http://localhost:8000/agents/web-agent/chat",
    {"message": "Quick question"}
)

print(f"Total time: {metrics['client_time']:.2f}s")
print(f"Server processing: {metrics['server_time']:.2f}s")
print(f"Network overhead: {metrics['network_overhead']:.2f}s")
```

### Database Direct Access

For advanced use cases, access PostgreSQL directly:

```python
import psycopg
from os import getenv

conn = psycopg.connect(
    host=getenv("DB_HOST", "localhost"),
    port=getenv("DB_PORT", "5432"),
    user=getenv("DB_USER", "ai"),
    password=getenv("DB_PASS", "ai"),
    dbname=getenv("DB_DATABASE", "ai")
)

# Query agent conversation history
with conn.cursor() as cur:
    cur.execute("""
        SELECT * FROM agent_runs
        WHERE agent_id = 'advanced-web-research-agent'
        ORDER BY created_at DESC
        LIMIT 10
    """)
    for row in cur.fetchall():
        print(row)

conn.close()
```

---

## Troubleshooting

### Common Issues

#### 1. Connection Refused
**Problem**: `Connection refused` or `Failed to connect`

**Solutions**:
- Check if the service is running: `curl http://localhost:8000/health`
- Verify Docker containers: `docker compose ps`
- Check logs: `docker compose logs api`

#### 2. Database Connection Errors
**Problem**: `Database connection failed`

**Solutions**:
- Verify PostgreSQL is running: `docker compose ps pgvector`
- Check environment variables: `echo $DB_HOST $DB_PORT`
- Test database connection: `docker compose exec pgvector psql -U ai`

#### 3. API Key Errors
**Problem**: `OpenAI API key not found` or similar

**Solutions**:
- Verify environment variable: `echo $OPENAI_API_KEY`
- Check `.env` file if using one
- Restart application after setting variables

#### 4. Timeout Errors
**Problem**: Request times out before completion

**Solutions**:
- Increase client timeout (agents: 60s, teams: 120s, workflows: 180s)
- Check agent logs for processing issues
- Consider using streaming mode for long responses

#### 5. 422 Validation Error
**Problem**: `Unprocessable Entity` response

**Solutions**:
- Verify request JSON structure
- Ensure required fields (`message`) are present
- Check field types (e.g., `stream` should be boolean)

### Debug Mode

Enable debug logging:

```bash
export DEBUG_MODE=true
docker compose restart api

# View detailed logs
docker compose logs -f api
```

### Health Check Script

```bash
#!/bin/bash
# health_check.sh

echo "Checking API health..."
curl -s http://localhost:8000/health | jq '.'

echo -e "\nChecking available agents..."
curl -s http://localhost:8000/agents | jq '.agents[].id'

echo -e "\nChecking database connection..."
docker compose exec pgvector pg_isready -U ai

echo -e "\nHealth check complete!"
```

---

## Deployment Considerations

### Production Checklist

- [ ] Use managed PostgreSQL service (AWS RDS, Google Cloud SQL)
- [ ] Implement API authentication (API keys, JWT, OAuth)
- [ ] Enable HTTPS/TLS with valid certificates
- [ ] Set up monitoring and alerting (Prometheus, Grafana)
- [ ] Configure log aggregation (ELK, CloudWatch)
- [ ] Implement rate limiting and request throttling
- [ ] Use environment-specific configurations
- [ ] Set up CI/CD pipeline for deployments
- [ ] Configure auto-scaling based on load
- [ ] Implement backup and disaster recovery
- [ ] Enable CORS with specific origins
- [ ] Add request/response size limits
- [ ] Set up health check endpoints for load balancers
- [ ] Configure timeouts appropriately
- [ ] Monitor LLM API costs and usage

### Recommended Architecture

```
Internet → Load Balancer (HTTPS) → API Instances (Auto-scaling)
                                          ↓
                            Managed PostgreSQL + Replica
                                          ↓
                                  LLM Provider APIs
```

---

## Additional Resources

### Documentation
- **Agno Framework**: https://docs.agno.com
- **FastAPI**: https://fastapi.tiangolo.com
- **PostgreSQL**: https://www.postgresql.org/docs/
- **pgvector**: https://github.com/pgvector/pgvector

### Community
- **Discord**: https://agno.link/discord
- **Discourse**: https://agno.link/community
- **GitHub Issues**: https://github.com/agno-agi/agent-api/issues

### API Testing Tools
- **Swagger UI**: http://localhost:8000/docs
- **Postman**: Import OpenAPI schema from `/openapi.json`
- **HTTPie**: `http POST localhost:8000/agents/web-agent/chat message="test"`
- **Insomnia**: REST client with collection support

---

## Changelog

### Version 1.0.0 (Current)
- Initial API documentation
- 6 specialized agents
- 1 research team (4-agent coordination)
- 2 automated workflows
- Multi-model support (OpenAI, DeepSeek, GLM, Gemini)
- PostgreSQL + pgvector integration
- Docker Compose deployment
- Comprehensive testing suite

---

## Support

For issues, questions, or contributions:

1. **Check Documentation**: Review this API documentation and Agno docs
2. **Search Existing Issues**: GitHub issues for similar problems
3. **Enable Debug Mode**: Get detailed logs for troubleshooting
4. **Community Support**: Discord and Discourse for help
5. **Report Bugs**: Create GitHub issue with reproduction steps

---

**Last Updated**: 2025-11-13
**API Version**: 1.0.0
**Framework**: Agno 2.0.4
**License**: See repository LICENSE file
